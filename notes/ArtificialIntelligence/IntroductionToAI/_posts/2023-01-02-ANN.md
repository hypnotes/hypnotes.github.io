---
layout: post
title: ANN Theory
description: >
  DAY 5 - 6
# image: /assets/img/blog/example-content-ii.jpg
sitemap: false
permalink: /notes/IntroductionToAI/ANN
---

* this list will be replaced by the toc
{:toc .large-only}

> ANN: Aritificial Neural Network

## Introduction

- Human Brain (Neuron) to Deep Learning Model via mathematical modeling (Ï†ïÎ≥¥Ï†ÑÎã¨Í≥ºÏ†ï)

<br/><img src="../ArtificialIntelligence/IntroductionToAI/assets/5-neuron.png" alt="NeuronToBC" style="height: 300px; width: auto;"/>

- inputs can be modified by weights
  - amplification, decrease, or eliminated by *0
- result activated if passes threashold (BINARY CLASSIFICATION)

## Multilayer Perception (MLP)
> Proposed and Mathematically proven by Prof. Marvin Minsky at MIT (1969), "Father of AI" (who first made the term AI)

- NEURAL NETWORK ARCHITECTURE 
- NN : **Do Linear Classification a lot of times**
<br/><img src="../ArtificialIntelligence/IntroductionToAI/assets/5-multilayer.png" alt="NeuronToBC" style="height: 400px; width: auto;"/>

- 3-size inputs [x, y, z] => 1-size output 
  - 1 size output: linear regression or binary classification
  - 2 or more: softmax classification
- Hidden Layers do additional Linear Classifications
- 3 Linear Classifications (indicating the model is nonlinear) => complex calculations are possible 

### Application to Logic Gate Design 

- **AND, OR** Gates
  -  Binary Classification is possible)
  - <img src="../ArtificialIntelligence/IntroductionToAI/assets/5-andor.png" alt="NeuronToBC" style="height: 300px; width: auto;"/>
  - points can be divided into 2 parts depending on y [0, 1]. (red, blue) 

- **XOR** Gate
> XOR Gate = Same ? 0 : 1
  - ***why hidden layer was first created***
  - <img src="../ArtificialIntelligence/IntroductionToAI/assets/5-xor.png" alt="NeuronToBC" style="height: 300px; width: auto;"/>
  - **<fontcolor>requires 2 linear classification</fontcolor>**

### Solving XOR with MLP

- $$\bar y$$ = **XOR** 

| $$x_1$$ | $$x_2$$ | $$y_1$$ | $$y_2$$ | $$\bar y$$ | **XOR** |
|:-------:|:-------:|:-------:|:-------:|:----------:|:-------:|
| 0 | 0 | 0 | 1 | 0 | 0 |
| 0 | 1 | 0 | 0 | 1 | 1 |
| 1 | 0 | 0 | 0 | 1 | 1 |
| 1 | 1 | 1 | 0 | 0 | 0 |

- proves that hidden layers allow unsolvable problems solvable 

<img src="../ArtificialIntelligence/IntroductionToAI/assets/5-solvexor.png" alt="NeuronToBC" style="height: 200px; width: auto;"/>



<details>                   
<summary><strong><fontcolor>(x1 x2) = (0 0)</fontcolor></strong></summary>
<div markdown="1">

  - $$y_1 = (0\quad 0) \begin{pmatrix} 5 \\ 5 \end{pmatrix}	+ (-8) = -8$$ ‚û© $$Sigmoid(-8) \approx 0$$;
  - $$y_2 = (0\quad 0) \begin{pmatrix} -7 \\ -7 \end{pmatrix}	+ (3) = 3$$ ‚û© $$Sigmoid(3) \approx 1$$; 
  - $$(y_1\quad y_2) \begin{pmatrix} -11 \\ -11 \end{pmatrix}	+ (6) = -11 + 6 = -5$$ ‚û© $$Sigmoid(-5) \approx 0$$


</div></details>

<details>                   
<summary><strong><fontcolor>(x1 x2) = (0 1)</fontcolor></strong></summary>
<div markdown="1">

  - $$y_1 = (0\quad 1) \begin{pmatrix} 5 \\ 5 \end{pmatrix}	+ (-8) = -3$$ ‚û© $$Sigmoid(-3) \approx 0$$; $$y_1 = 0$$
  - $$y_2 = (0\quad 1) \begin{pmatrix} -7 \\ -7 \end{pmatrix}	+ (3) = -4$$ ‚û© $$Sigmoid(-4) \approx 0$$; $$y_2 = 0$$
  - $$(y_1\quad y_2) \begin{pmatrix} -11 \\ -11 \end{pmatrix}	+ (6) = 0 + 6 = 6$$ ‚û© $$Sigmoid(6) \approx 1$$

</div></details>

<details>                   
<summary> <strong><fontcolor>(x1 x2) = (1 0)</fontcolor></strong> </summary>
<div markdown="1">

  - $$y_1 = (0\quad 0) \begin{pmatrix} 5 \\ 5 \end{pmatrix}	+ (-8) = -3$$ ‚û© $$Sigmoid(-3) \approx 0$$; $$y_1 = 0$$
  - $$(y_2 = 0\quad 0) \begin{pmatrix} -7 \\ -7 \end{pmatrix}	+ (3) = -4$$ ‚û© $$Sigmoid(-4) \approx 1$$; $$y_2 = 0$$
  - $$(y_1\quad y_2) \begin{pmatrix} -11 \\ -11 \end{pmatrix}	+ (6) = 0 + 6 = -5$$ ‚û© $$Sigmoid(6) \approx 1$$


</div></details>
<details>                   
<summary> <strong><fontcolor>(x1 x2) = (1 1)</fontcolor></strong> </summary>
<div markdown="1">

  - $$y_1 = (0\quad 0) \begin{pmatrix} 5 \\ 5 \end{pmatrix}	+ (-8) = 2$$ ‚û© $$Sigmoid(2) \approx 1$$; 
  - $$y_1 = (0\quad 0) \begin{pmatrix} -7 \\ -7 \end{pmatrix}	+ (3) = -11$$ ‚û© $$Sigmoid(-11) \approx 0$$;
  - $$(y_1\quad y_2) \begin{pmatrix} -11 \\ -11 \end{pmatrix}	+ (6) = -11 + 6 = -5$$ ‚û© $$Sigmoid(-5) \approx 0$$


</div></details>

### **Forward Propogation** 

- can we add another hidden layer? 
<br/>
<img src="../ArtificialIntelligence/IntroductionToAI/assets/5-xoradd.png" alt="NeuronToBC" style="height: 200px; width: auto;"/>
<br/><br/>

- Then new W $$\begin{pmatrix} ? \\ ? \end{pmatrix}$$ and $$b$$ required for new $$S$$ and $$W=\begin{pmatrix} -11 \\ -11 \end{pmatrix}$$ (red box) ‚û© $$W = \begin{pmatrix} -11 \\ -11 \\ ? \end{pmatrix}$$
<br/>
<img src="../ArtificialIntelligence/IntroductionToAI/assets/5-ksize.png" alt="NeuronToBC" style="height: 300px; width: auto;"/>

### Toy Model

<img src="../ArtificialIntelligence/IntroductionToAI/assets/6-toymodel.png" alt="ToyModel" style="height: 400px; width: auto;"/>

- although *hidden layer 1* and *hidden layer 2* looks alike, their weight vectors have the different size (different size inputs)

## PyTorch implementation for ANN (XOR)üî•

```py
import torch
import numpy as np

# Training Data
x_train = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]) #XOR DATA
y_train = torch.FloatTensor([[0], [1], [1], [0]])

nHL = 3

W_h = torch.randn([2, nHL], requires_grad=True) # hidden layer weight
b_h = torch.randn([nHL], requires_grad = True)  
W_o = torch.randn([nHL, 1], requires_grad=True) # ouput layer weight
b_o = torch.randn([1], requires_grad=True)

optimizer = torch.optim.SGD([W_h, W_o, b_h, b_o], lr = 0.01) 

def model_ANN(x):
  HL1 = torch.sigmoid(torch.matmul(x, W_h) + b_h) #hidden layer with 3 units (Î®ºÏ†Ä ÏÉùÏÑ±)
  Out = torch.sigmoid(torch.matmul(HL1, W_o) + b_o)
  return Out

for step in range(200000):
  prediction = model_ANN(x_train)
  cost = torch.mean( (-1) * ((y_train*torch.log(prediction) + (1-y_train)*torch.log(1-prediction))))
  optimizer.zero_grad() # 0ÍπåÏßÄ optimize
  cost.backward()       
  optimizer.step()

model_test = model_ANN(x_train)
print(model_test.detach().numpy())
```

### Code Explanation

```py
nHL = 3

W_h = torch.randn([2, nHL], requires_grad=True) # hidden layer weight
b_h = torch.randn([nHL], requires_grad = True)  
```

<img src="../ArtificialIntelligence/IntroductionToAI/assets/6-codeexp.png" alt="CodeExplanation" style="height: 200px; width: auto;"/>



```py
def model_ANN(x):
  HL1 = torch.sigmoid(torch.matmul(x, W_h) + b_h) #hidden layer with 3 units (Î®ºÏ†Ä ÏÉùÏÑ±)
  Out = torch.sigmoid(torch.matmul(HL1, W_o) + b_o)
  return Out
```
- input = [ [0, 0], [0, 1], [1, 0], [1, 1] ]

- **Hidden Layer Ï∂îÍ∞Ä ÏïàÌï† Ïãú** output:
  - <code>[ 0.5, 0.5, 0.5, 0.5]</code> => all are not <code>> 0.5 </code> 
    - => <code>[ 0, 0, 0, 0 ] </code>
    - ACTUAL: [ 0, 1, 1, 0 ] (50% accuracy) => HIDDEN LAYER REQUIRED

- **Hidden Layer** output :
  - <code>[ 0.001413, 0.9953.., 0.993166..., 0.0079 ]</code> => <code>[ 0, 1, 1, 0 ] </code> => CORRECT

## Further ANNs 

### Wide ANN for XOR

(Ï∞∏Í≥†: tensorflow)<br/>
<img src="../ArtificialIntelligence/IntroductionToAI/assets/6-wideann.png" alt="CodeExplanation" style="height: 200px; width: auto;"/>

### Deep ANN for XOR

(Ï∞∏Í≥†: tensorflow)<br/>
<img src="../ArtificialIntelligence/IntroductionToAI/assets/6-deepann.png" alt="CodeExplanation" style="height: 300px; width: auto;"/>

- **More Layer the BETTER**

## Gradient Vanishing Problem

<img src="../ArtificialIntelligence/IntroductionToAI/assets/6-annvanshi.png" alt="CodeExplanation" style="height: 100px; width: auto;"/>

- No matter high the number of layers, accuracy can be low
- ex) 100000 as input -> mapped to 0 ~ 1 -> REPEAT -> ... -> Value disappears (converges to 0)

<img src="../ArtificialIntelligence/IntroductionToAI/assets/6-vanish.png" alt="CodeExplanation" style="height: 100px; width: auto;"/>

### RELU (Rectified Linear Unit)

- solves the Gradient Vanishing Problem <br/>

<img src="../ArtificialIntelligence/IntroductionToAI/assets/6-relu.png" alt="CodeExplanation" style="height: 300px; width: auto;"/>

- when activated, the actual value is returned. ex) input 3, returns 3

### Deep Learning Revolution

|50 Years Ago | Now |
|:------------|:------------------------|
|labeled datasets too small | Big - Data |
|Computers too slow | GPU |
|only consider 1-D vector input | Conbolutional Layers for n-D inputs (Hidden Layers) |
|wrong type of non-linearity (activation function) | ReLU for Gradient Vanishing Problem|

## Deep Learning Review

### Deep Learning Computation Procedure

1. Deep Learning Model Setup 
  - MLP, CNN, RNN, GAN, Costomized Ï§ë Î≠ê Ïì∏ Í≤ÉÏù∏ÏßÄ..
  - Number of Hidden Layers, Units, Input/Outputs...
  - Cost Function / Optimizer Selection

2. Training (with Large-Scale Dataset)
  - Input Data, Output: Labels
  - Learning -> Weights Updates ($$W$$ and $$b$$) for Cost Function Minimization

3. Inference / Testing (Real-World Execution)
  - Use $$W$$ and $$b$$ (optimized at step #2) to calculate input
  - Input : Real-World Input Data
  - Output: Inference Results based on Updated Weights in Deep NN

