---
layout: post
title: ANN Theory
description: >
  DAY 5 - 6
# image: /assets/img/blog/example-content-ii.jpg
sitemap: false
permalink: /notes/IntroductionToAI/ANN
---

* this list will be replaced by the toc
{:toc .large-only}

> ANN: Aritificial Neural Network

## Introduction

- Human Brain (Neuron) to Deep Learning Model via mathematical modeling (정보전달과정)

<br/><img src="../ArtificialIntelligence/IntroductionToAI/assets/5-neuron.png" alt="NeuronToBC" style="height: 300px; width: auto;"/>

- inputs can be modified by weights
  - amplification, decrease, or eliminated by *0
- result activated if passes threashold (BINARY CLASSIFICATION)

## Multilayer Perception (MLP)
> Proposed and Mathematically proven by Prof. Marvin Minsky at MIT (1969), "Father of AI" (who first made the term AI)

- NEURAL NETWORK ARCHITECTURE 
- NN : **Do Linear Classification a lot of times**
<br/><img src="../ArtificialIntelligence/IntroductionToAI/assets/5-multilayer.png" alt="NeuronToBC" style="height: 400px; width: auto;"/>

- 3-size inputs [x, y, z] => 1-size output 
  - 1 size output: linear regression or binary classification
  - 2 or more: softmax classification
- Hidden Layers do additional Linear Classifications
- 3 Linear Classifications (indicating the model is nonlinear) => complex calculations are possible 

### Application to Logic Gate Design 

- **AND, OR** Gates
  -  Binary Classification is possible)
  - <img src="../ArtificialIntelligence/IntroductionToAI/assets/5-andor.png" alt="NeuronToBC" style="height: 300px; width: auto;"/>
  - points can be divided into 2 parts depending on y [0, 1]. (red, blue) 

- **XOR** Gate
> XOR Gate = Same ? 0 : 1
  - ***why hidden layer was first created***
  - <img src="../ArtificialIntelligence/IntroductionToAI/assets/5-xor.png" alt="NeuronToBC" style="height: 300px; width: auto;"/>
  - **<fontcolor>requires 2 linear classification</fontcolor>**

### Solving XOR with MLP

- $$\bar y$$ = **XOR** 

| $$x_1$$ | $$x_2$$ | $$y_1$$ | $$y_2$$ | $$\bar y$$ | **XOR** |
|:-------:|:-------:|:-------:|:-------:|:----------:|:-------:|
| 0 | 0 | 0 | 1 | 0 | 0 |
| 0 | 1 | 0 | 0 | 1 | 1 |
| 1 | 0 | 0 | 0 | 1 | 1 |
| 1 | 1 | 1 | 0 | 0 | 0 |

- proves that hidden layers allow unsolvable problems solvable 

<img src="../ArtificialIntelligence/IntroductionToAI/assets/5-solvexor.png" alt="NeuronToBC" style="height: 200px; width: auto;"/>



<details>                   
<summary><strong><fontcolor>(x1 x2) = (0 0)</fontcolor></strong></summary>
<div markdown="1">

  - $$y_1 = (0\quad 0) \begin{pmatrix} 5 \\ 5 \end{pmatrix}	+ (-8) = -8$$ ➩ $$Sigmoid(-8) \approx 0$$;
  - $$y_2 = (0\quad 0) \begin{pmatrix} -7 \\ -7 \end{pmatrix}	+ (3) = 3$$ ➩ $$Sigmoid(3) \approx 1$$; 
  - $$(y_1\quad y_2) \begin{pmatrix} -11 \\ -11 \end{pmatrix}	+ (6) = -11 + 6 = -5$$ ➩ $$Sigmoid(-5) \approx 0$$


</div></details>

<details>                   
<summary><strong><fontcolor>(x1 x2) = (0 1)</fontcolor></strong></summary>
<div markdown="1">

  - $$y_1 = (0\quad 1) \begin{pmatrix} 5 \\ 5 \end{pmatrix}	+ (-8) = -3$$ ➩ $$Sigmoid(-3) \approx 0$$; $$y_1 = 0$$
  - $$y_2 = (0\quad 1) \begin{pmatrix} -7 \\ -7 \end{pmatrix}	+ (3) = -4$$ ➩ $$Sigmoid(-4) \approx 0$$; $$y_2 = 0$$
  - $$(y_1\quad y_2) \begin{pmatrix} -11 \\ -11 \end{pmatrix}	+ (6) = 0 + 6 = 6$$ ➩ $$Sigmoid(6) \approx 1$$

</div></details>

<details>                   
<summary> <strong><fontcolor>(x1 x2) = (1 0)</fontcolor></strong> </summary>
<div markdown="1">

  - $$y_1 = (0\quad 0) \begin{pmatrix} 5 \\ 5 \end{pmatrix}	+ (-8) = -3$$ ➩ $$Sigmoid(-3) \approx 0$$; $$y_1 = 0$$
  - $$(y_2 = 0\quad 0) \begin{pmatrix} -7 \\ -7 \end{pmatrix}	+ (3) = -4$$ ➩ $$Sigmoid(-4) \approx 1$$; $$y_2 = 0$$
  - $$(y_1\quad y_2) \begin{pmatrix} -11 \\ -11 \end{pmatrix}	+ (6) = 0 + 6 = -5$$ ➩ $$Sigmoid(6) \approx 1$$


</div></details>
<details>                   
<summary> <strong><fontcolor>(x1 x2) = (1 1)</fontcolor></strong> </summary>
<div markdown="1">

  - $$y_1 = (0\quad 0) \begin{pmatrix} 5 \\ 5 \end{pmatrix}	+ (-8) = 2$$ ➩ $$Sigmoid(2) \approx 1$$; 
  - $$y_1 = (0\quad 0) \begin{pmatrix} -7 \\ -7 \end{pmatrix}	+ (3) = -11$$ ➩ $$Sigmoid(-11) \approx 0$$;
  - $$(y_1\quad y_2) \begin{pmatrix} -11 \\ -11 \end{pmatrix}	+ (6) = -11 + 6 = -5$$ ➩ $$Sigmoid(-5) \approx 0$$


</div></details>

### **Forward Propogation** 

- can we add another hidden layer? 
<br/>
<img src="../ArtificialIntelligence/IntroductionToAI/assets/5-xoradd.png" alt="NeuronToBC" style="height: 200px; width: auto;"/>
<br/><br/>

- Then new W $$\begin{pmatrix} ? \\ ? \end{pmatrix}$$ and $$b$$ required for new $$S$$ and $$W=\begin{pmatrix} -11 \\ -11 \end{pmatrix}$$ (red box) ➩ $$W = \begin{pmatrix} -11 \\ -11 \\ ? \end{pmatrix}$$
<br/>
<img src="../ArtificialIntelligence/IntroductionToAI/assets/5-ksize.png" alt="NeuronToBC" style="height: 300px; width: auto;"/>