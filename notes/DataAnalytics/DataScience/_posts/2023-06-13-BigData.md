---
layout: post
title: 14 Big Data
description: >

# image: ../DataAnalytics/DataScience/assets/10-title.jpg
hide_image: true
sitemap: false
permalink: /notes/DataScience/BigData
---

- this list will be replaced by the toc
{:toc .large-only}

## Review: Taxonomy of ML
1. Supervised Learning (Labeled Data)
  - **_Regression_** : Quantitative Response <fade>still mostly used for classification...</fade>
  - **_Classification_** : Categorical Response

2. <cb>Unsupervised Learning</cb> (Unlabeled Data)
  - **_Dimensionality Reduction_**
  - **_Clustering_**
    - example: Netflix, Reverse Engineering Biology

## K-Means Clustering Algorithm
- most popular clustering approach
1. pick arbitrary $$k$$, randomly place $$k$$ "centers", each a different color
2. repeat until convergence:
  - ***color points*** according to the closest center
  - ***move center*** for each color to center of points with that color

<img src="../DataAnalytics/DataScience/assets/13-kmeans.gif" alt="kmeans" style="height: 300px; width: auto;"/>

- iteration 4 and 5: Centers moved slightly (no points changed color)
- iteration 5 and 6 (not shown): no change $$\Rightarrow$$ END

- [참고] K-Means $$\neq$$ K-Nearest Neighbors
  - **K-Means**: Clustering (assigns each point to one of $$K$$ clusters)
  - **K-Nearest Neighbors**: Classification (or Regression)

## Minimizing Inertia
- K-Means Clustering for $$K=4$$ : each run different output
<img src="../DataAnalytics/DataScience/assets/13-kmeanss.png" alt="kmeans" style="height: 200px; width: auto;"/>

- Need Loss Function to determine BEST

- Intracluster Distance (distance within a cluster) $$<$$ Intercluster Distance (distance between other clusters)


- **Loss Functions**:

<img src="../DataAnalytics/DataScience/assets/13-example.png" alt="kmeans" style="height: 200px; width: auto;"/>

1. <cb>Inertia</cb>: Sum of squared distance from each data point to its center
  - $$0.47^2 + 0.19^2+0.34^2 + 0.25^2 + 0.58^2 + 0.36^2 + 0.44^2$$ $$$$
  - lower the better 
2. <cb>Distortion</cb>: weighted sum of squared distances from each data point to tis center
  - $$ \frac{0.472 + 0.192 + 0.342}{3} + \frac{0.252 + 0.582 + 0.362 + 0.442}{4}$$ $$$$

<img src="../DataAnalytics/DataScience/assets/13-inertia.png" alt="inertia" style="height: 200px; width: auto;"/>

- $$\Rightarrow$$ Leftmost (`44.96`): BEST, Rightmost (`54.35`): WORST
- **K-Means try to minimize inertia** but often fails to find global optimum
  - K Means: 2개의 optimizer이 번갈아가면서 수행한다고 생각하면 됨
  1. First optimizer $$\rightarrow$$ center position: `hold`, data colors: $$optimize$$
  2. Second optimizer $$\rightarrow$$ center position:  $$optimize$$, data colors:`hold`
  - $$\Rightarrow$$ neither gets total control: why we iterate

- best algorithm so far: 
  - for all possible $$k^n$$ coloring:
    - compute $$k$$ centers for coloring
    - compute **inertia** for $$k$$ centers
      - `if` `current_inertia` better than `best_known`: 
        - `best_known` $$\leftarrow$$ `current_inertia`
- $$\Rightarrow$$ 안쓰는 이유: $$k^n$$ too big to compute
- **inertia** will only show local instead of global
- no better algorithm found **K-Means = NP-hard**

## Agglomerative Clustering
- aka hierarchical clustering 

<img src="../DataAnalytics/DataScience/assets/11-noise.png" alt="nestrov" style="height: 200px; width: auto;"/>

## Picking K


<details>
  <summary>Full algorithm</summary>
  <div markdown="1">

  <img src="../DataAnalytics/DataScience/assets/11-noise.png" alt="nestrov" style="height: 200px; width: auto;"/>
</div></details>
